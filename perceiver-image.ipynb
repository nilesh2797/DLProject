{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3c640c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:03.637335Z",
     "start_time": "2021-11-30T03:03:03.627691Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca772133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:05.241456Z",
     "start_time": "2021-11-30T03:03:05.239886Z"
    }
   },
   "outputs": [],
   "source": [
    "# downlaod deepmind's pretrained language model\n",
    "# !wget -O deepmind_assets/language_perceiver_io_bytes.pickle https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
    "# !wget -O deepmind_assets/imagenet_perceiver.pystate https://storage.googleapis.com/perceiver_io/imagenet_conv_preprocessing.pystate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa32b428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:09.233486Z",
     "start_time": "2021-11-30T03:03:06.669732Z"
    }
   },
   "outputs": [],
   "source": [
    "from perceiver_io.perceiver_lm import PerceiverLM\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm.notebook import tqdm\n",
    "from deepmind_assets import bytes_tokenizer\n",
    "import xclib.evaluation.xc_metrics as xc_metrics\n",
    "from utils import csr_to_pad_tensor, ToD, read_sparse_mat, XCMetrics, _c\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The tokenizer is just UTF-8 encoding (with an offset)\n",
    "tokenizer = bytes_tokenizer.BytesTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b3fcf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:09.239801Z",
     "start_time": "2021-11-30T03:03:09.235820Z"
    }
   },
   "outputs": [],
   "source": [
    "command = \"--dataset cifar10\"\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project', default='PerceiverIO')\n",
    "parser.add_argument('--dataset', default='EURLex-4K')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "\n",
    "args = parser.parse_args(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cddcb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:09.244958Z",
     "start_time": "2021-11-30T03:03:09.240960Z"
    }
   },
   "outputs": [],
   "source": [
    "args.expname = f'{args.project}-image'\n",
    "args.maxlen = 2048\n",
    "\n",
    "args.n_epochs = 10\n",
    "args.lr = 5e-4\n",
    "args.bsz = 16\n",
    "args.dropout = 0.5\n",
    "args.warmup = 0.1\n",
    "args.loss_with_logits = True\n",
    "args.amp = True\n",
    "args.eval_interval = 1\n",
    "\n",
    "OUT_DIR = f'Results/{args.expname}/{args.dataset}/IN'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74fead44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:09.251718Z",
     "start_time": "2021-11-30T03:03:09.246747Z"
    }
   },
   "outputs": [],
   "source": [
    "args.img_size = 32\n",
    "if args.dataset == 'tiny-imagenet':\n",
    "    args.img_size = 64\n",
    "elif args.dataset == 'stl10':\n",
    "    args.img_size = 96\n",
    "elif args.dataset == 'cifar10':\n",
    "    args.img_size = 32\n",
    "    \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(args.img_size),\n",
    "    transforms.RandomCrop(args.img_size, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(args.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ca9b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:10.169000Z",
     "start_time": "2021-11-30T03:03:09.253054Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.dataset == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)\n",
    "    args.numy = 10\n",
    "elif args.dataset == 'tiny-imagenet':\n",
    "    fix_tin_val_folder('./data/tiny-imagenet-200/val')\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/train', transform=transform_train)\n",
    "    testset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/val', transform=transform_test)\n",
    "    args.numy = 200\n",
    "elif args.dataset == 'stl10':\n",
    "    trainset = torchvision.datasets.STL10(root='./data', split='train', download=False, transform=transform_train)\n",
    "    testset = torchvision.datasets.STL10(root='./data', split='test', download=False, transform=transform_test)\n",
    "    args.numy = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd29aa79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:10.172213Z",
     "start_time": "2021-11-30T03:03:10.170191Z"
    }
   },
   "outputs": [],
   "source": [
    "# args.vocab_size = 262\n",
    "# args.embed_dim = args.latent_dim = 768\n",
    "# args.num_latents = 256\n",
    "# args.per_label_task = False\n",
    "# args.per_token_decoder = False\n",
    "\n",
    "# encoder = PerceiverLM(vocab_size=args.vocab_size, \n",
    "#                       max_seq_len=args.maxlen, \n",
    "#                       embedding_dim=args.embed_dim, \n",
    "#                       num_latents=args.num_latents, \n",
    "#                       latent_dim=1280, \n",
    "#                       qk_out_dim=256, \n",
    "#                       dropout=0,\n",
    "#                       num_self_attn_per_block=26, \n",
    "#                       per_token_decoder=args.per_token_decoder, \n",
    "#                       num_query_tasks=args.numy if args.per_label_task else 1)\n",
    "\n",
    "# encoder.load_pretrained(\"deepmind_assets/language_perceiver_io_bytes.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63396344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.541452Z",
     "start_time": "2021-11-30T03:03:10.173149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# import jax\n",
    "# import haiku\n",
    "\n",
    "# import pickle\n",
    "# with open('deepmind_assets/imagenet_perceiver.pystate', \"rb\") as f:\n",
    "#     params = pickle.loads(f.read())\n",
    "\n",
    "# params['params']['perceiver_encoder/~/self_attention/attention/linear']['w'].shape\n",
    "# list(params['params'].keys())\n",
    "\n",
    "args.per_label_task = False\n",
    "args.per_token_decoder = False\n",
    "args.num_latents = 512\n",
    "args.latent_dim = 1024\n",
    "args.embed_dim = 322\n",
    "\n",
    "from perceiver_io.perceiver_in import PerceiverIN\n",
    "encoder = PerceiverIN(num_blocks=2)\n",
    "encoder.load_pretrained('deepmind_assets/imagenet_perceiver.pystate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2af6273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.548853Z",
     "start_time": "2021-11-30T03:03:11.542755Z"
    }
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9556e1b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.552118Z",
     "start_time": "2021-11-30T03:03:11.549829Z"
    }
   },
   "outputs": [],
   "source": [
    "args.patch_height = 4\n",
    "args.patch_width = 4\n",
    "args.num_patches = (args.img_size // args.patch_height) * (args.img_size // args.patch_width)\n",
    "args.patch_dim = 3 * args.patch_height * args.patch_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46123c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.556471Z",
     "start_time": "2021-11-30T03:03:11.553651Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.bsz, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=args.bsz, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aefffc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.567313Z",
     "start_time": "2021-11-30T03:03:11.557391Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, args):\n",
    "        super().__init__()\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = args.patch_height, p2 = args.patch_width),\n",
    "            nn.Linear(args.patch_dim, args.embed_dim),\n",
    "        )\n",
    "        self.encoder = encoder\n",
    "        self.position_embedding = self.encoder.position_embedding if hasattr(self.encoder, 'position_embedding') else nn.Embedding(args.num_patches, args.embed_dim)\n",
    "        self.numy = args.numy\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        if args.per_label_task:\n",
    "            self.w = nn.Sequential(nn.Linear(args.embed_dim, 2*args.embed_dim), \n",
    "                                   nn.ReLU(), \n",
    "                                   nn.Linear(2*args.embed_dim, 1))\n",
    "        else:\n",
    "            self.w = nn.Linear(args.latent_dim, args.numy)\n",
    "        \n",
    "    def get_device(self):\n",
    "        return list(self.parameters())[0].device\n",
    "    \n",
    "    def forward(self, b):\n",
    "        patch_embs = self.to_patch_embedding(b)\n",
    "        seq_len = patch_embs.size(1)\n",
    "        batch_size = patch_embs.size(0)\n",
    "        \n",
    "        pos_ids = torch.arange(seq_len, device=patch_embs.device).view(1, -1)\n",
    "        pos_embs = self.position_embedding(pos_ids)\n",
    "        embs = patch_embs + pos_embs\n",
    "        \n",
    "        if args.per_token_decoder:\n",
    "            query_embs = self.encoder.query_position_embedding(pos_ids).repeat(batch_size, 1, 1)\n",
    "            query_mask = None\n",
    "        else:\n",
    "            query_embs = self.encoder.query_task_embedding.weight.repeat(batch_size, 1, 1)\n",
    "            query_mask = None\n",
    "            \n",
    "        embs = self.encoder.perceiver(\n",
    "            inputs=embs,\n",
    "            query=query_embs,\n",
    "            input_mask=None,\n",
    "            query_mask=None\n",
    "        )\n",
    "        \n",
    "        if self.encoder.per_token_decoder:\n",
    "            embs = embs.mean(dim=1)\n",
    "        else:\n",
    "            embs = embs.squeeze()\n",
    "    \n",
    "        out = self.w(self.dropout(embs))\n",
    "        return out.squeeze()\n",
    "    \n",
    "class OvABCELoss(nn.Module):\n",
    "    def __init__(self, args, reduction='mean'):\n",
    "        super(OvABCELoss, self).__init__()\n",
    "        if args.loss_with_logits:\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        else:\n",
    "            self.criterion = torch.nn.BCELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, model, b):\n",
    "        out = model(b)\n",
    "        targets = torch.zeros((out.shape[0], out.shape[1]+1), device=out.device).scatter_(1, b['y']['inds'], 1)[:, :-1]\n",
    "        loss = self.criterion(out, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b36845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.572116Z",
     "start_time": "2021-11-30T03:03:11.568208Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net(encoder, args)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820f5a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:11.582451Z",
     "start_time": "2021-11-30T03:03:11.573019Z"
    }
   },
   "outputs": [],
   "source": [
    "optims = [transformers.optimization.AdamW(net.parameters(), **{'lr': args.lr, 'eps': 1e-06, 'weight_decay': 0.01})]\n",
    "total_steps = len(trainloader)*args.n_epochs\n",
    "schedulers = [transformers.get_linear_schedule_with_warmup(optim, num_warmup_steps=int(args.warmup*total_steps), num_training_steps=total_steps) for optim in optims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84958ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:14.436338Z",
     "start_time": "2021-11-30T03:03:11.583341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
       "    (1): Linear(in_features=48, out_features=322, bias=True)\n",
       "  )\n",
       "  (encoder): PerceiverIN(\n",
       "    (query_task_embedding): Embedding(1, 1024)\n",
       "    (perceiver): PerceiverIO(\n",
       "      (encoder): PerceiverEncoder(\n",
       "        (cross_attn): CrossAttention(\n",
       "          (kv_layer_norm): LayerNorm((322,), eps=1e-05, elementwise_affine=True)\n",
       "          (q_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (k): Linear(in_features=322, out_features=322, bias=True)\n",
       "            (q): Linear(in_features=1024, out_features=322, bias=True)\n",
       "            (v): Linear(in_features=322, out_features=322, bias=True)\n",
       "            (projection): Linear(in_features=322, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (mlp): FeedForward(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (self_attention_block): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): MultiHeadAttention(\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (mlp): FeedForward(\n",
       "              (mlp): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): PerceiverDecoder(\n",
       "        (cross_attention): CrossAttention(\n",
       "          (kv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (q_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (qkv_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (k): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (mlp): FeedForward(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (projection): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (position_embedding): Embedding(64, 322)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (w): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c80d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T03:03:14.443683Z",
     "start_time": "2021-11-30T03:03:14.437852Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, testloader, epoch=-1):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    t = tqdm(testloader, desc='', leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(t):\n",
    "            inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "                outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            t.set_description(' '.join([str(batch_idx), str(len(testloader)), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total)]))\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    loss = test_loss/(batch_idx+1)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79193c72",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-30T03:03:11.421Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b65b1c3900b4ca5a6976504c67b06b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0, Loss: 0.0:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/08343/nilesh/maverick2/anaconda3/envs/xc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "best_acc = -100\n",
    "for epoch in range(args.n_epochs):\n",
    "    net.train()\n",
    "    cum_loss = 0; ctr = 0\n",
    "    t = tqdm(trainloader, desc='Epoch: 0, Loss: 0.0', leave=True)\n",
    "          \n",
    "    for b in t:        \n",
    "        for optim in optims: optim.zero_grad()\n",
    "        b = ToD({'input': b[0], 'label': b[1]}, args.device)\n",
    "        with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "            out = net(b['input'])\n",
    "        loss = criterion(out, b['label'])\n",
    "        \n",
    "        if args.amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            for optim in optims: scaler.step(optim)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            for optim in optims: optim.step()\n",
    "                \n",
    "        for sch in schedulers: sch.step()\n",
    "        cum_loss += loss.item()\n",
    "        ctr += 1\n",
    "        t.set_description('Epoch: %d/%d, Loss: %.4E'%(epoch, args.n_epochs, (cum_loss/ctr)), refresh=True)\n",
    "    \n",
    "    print(f'mean loss after epoch {epoch}/{args.n_epochs}: {\"%.4E\"%(cum_loss/ctr)}', flush=True)\n",
    "    if epoch%args.eval_interval == 0 or epoch == (args.n_epochs-1):\n",
    "        test_loss, test_acc = evaluate(net, testloader)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            print(f'Found new best model with acc: {\"%.2f\"%best_acc}\\n')\n",
    "            with open(f'{OUT_DIR}/log.txt', 'a') as f:\n",
    "                print(f'epoch: {epoch}, test acc: {test_acc}, train loss: {cum_loss/ctr}, test loss: {test_loss}', file=f)\n",
    "            torch.save(net.state_dict(), f'{OUT_DIR}/model.pt')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34b00476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T00:14:08.332576Z",
     "start_time": "2021-11-30T00:05:14.806256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a64a5feb854a94a760d709df989326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8536a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:38:58.932235Z",
     "start_time": "2021-11-18T21:38:58.913729Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5437/1918826831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnum_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for p in model.parameters():\n",
    "    num_params += np.prod(p.shape)\n",
    "\n",
    "num_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vit]",
   "language": "python",
   "name": "conda-env-vit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
