{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3c640c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:22.490444Z",
     "start_time": "2021-11-30T01:00:22.480304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca772133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:23.853613Z",
     "start_time": "2021-11-30T01:00:23.851901Z"
    }
   },
   "outputs": [],
   "source": [
    "# downlaod deepmind's pretrained language model\n",
    "# !wget -O deepmind_assets/language_perceiver_io_bytes.pickle https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa32b428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:26.729097Z",
     "start_time": "2021-11-30T01:00:24.076574Z"
    }
   },
   "outputs": [],
   "source": [
    "from perceiver_io.perceiver_lm import PerceiverLM\n",
    "\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm.notebook import tqdm\n",
    "from deepmind_assets import bytes_tokenizer\n",
    "import xclib.evaluation.xc_metrics as xc_metrics\n",
    "from utils import csr_to_pad_tensor, ToD, read_sparse_mat, XCMetrics, _c\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# The tokenizer is just UTF-8 encoding (with an offset)\n",
    "tokenizer = bytes_tokenizer.BytesTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b3fcf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:26.734279Z",
     "start_time": "2021-11-30T01:00:26.730648Z"
    }
   },
   "outputs": [],
   "source": [
    "command = \"--dataset cifar10\"\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--project', default='PerceiverIO')\n",
    "parser.add_argument('--dataset', default='EURLex-4K')\n",
    "parser.add_argument('--device', type=str, default='cuda:0')\n",
    "\n",
    "args = parser.parse_args(command.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cddcb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:26.741214Z",
     "start_time": "2021-11-30T01:00:26.735283Z"
    }
   },
   "outputs": [],
   "source": [
    "args.expname = f'{args.project}-image'\n",
    "args.maxlen = 2048\n",
    "args.vocab_size = 262\n",
    "args.embed_dim = 768\n",
    "args.num_latents = 256\n",
    "\n",
    "args.n_epochs = 10\n",
    "args.lr = 5e-4\n",
    "args.bsz = 16\n",
    "args.dropout = 0.5\n",
    "args.warmup = 0.1\n",
    "args.loss_with_logits = True\n",
    "args.amp = True\n",
    "args.eval_interval = 2\n",
    "\n",
    "args.per_label_task = False\n",
    "args.per_token_decoder = False\n",
    "\n",
    "OUT_DIR = f'Results/{args.expname}/{args.dataset}'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beff3f55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:26.748737Z",
     "start_time": "2021-11-30T01:00:26.742444Z"
    }
   },
   "outputs": [],
   "source": [
    "args.img_size = 32\n",
    "if args.dataset == 'tiny-imagenet':\n",
    "    args.img_size = 64\n",
    "elif args.dataset == 'stl10':\n",
    "    args.img_size = 96\n",
    "elif args.dataset == 'cifar10':\n",
    "    args.img_size = 32\n",
    "    \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(args.img_size),\n",
    "    transforms.RandomCrop(args.img_size, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(args.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ca9b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:27.647225Z",
     "start_time": "2021-11-30T01:00:26.749621Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.dataset == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform_test)\n",
    "    args.numy = 10\n",
    "elif args.dataset == 'tiny-imagenet':\n",
    "    fix_tin_val_folder('./data/tiny-imagenet-200/val')\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/train', transform=transform_train)\n",
    "    testset = torchvision.datasets.ImageFolder(root='./data/tiny-imagenet-200/val', transform=transform_test)\n",
    "    args.numy = 200\n",
    "elif args.dataset == 'stl10':\n",
    "    trainset = torchvision.datasets.STL10(root='./data', split='train', download=False, transform=transform_train)\n",
    "    testset = torchvision.datasets.STL10(root='./data', split='test', download=False, transform=transform_test)\n",
    "    args.numy = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd29aa79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:31.192466Z",
     "start_time": "2021-11-30T01:00:27.648506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['query_task_embedding.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "encoder = PerceiverLM(vocab_size=args.vocab_size, \n",
    "                      max_seq_len=args.maxlen, \n",
    "                      embedding_dim=args.embed_dim, \n",
    "                      num_latents=args.num_latents, \n",
    "                      latent_dim=1280, \n",
    "                      qk_out_dim=256, \n",
    "                      dropout=0,\n",
    "                      num_self_attn_per_block=26, \n",
    "                      per_token_decoder=args.per_token_decoder, \n",
    "                      num_query_tasks=args.numy if args.per_label_task else 1)\n",
    "\n",
    "encoder.load_pretrained(\"deepmind_assets/language_perceiver_io_bytes.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa72c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:31.201154Z",
     "start_time": "2021-11-30T01:00:31.194190Z"
    }
   },
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9839c661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:31.204789Z",
     "start_time": "2021-11-30T01:00:31.202178Z"
    }
   },
   "outputs": [],
   "source": [
    "args.patch_height = 4\n",
    "args.patch_width = 4\n",
    "args.num_patches = (args.img_size // args.patch_height) * (args.img_size // args.patch_width)\n",
    "args.patch_dim = 3 * args.patch_height * args.patch_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc4febb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:31.211282Z",
     "start_time": "2021-11-30T01:00:31.206177Z"
    }
   },
   "outputs": [],
   "source": [
    "## Sanity check for encoder with per_token_decoder=True\n",
    "\n",
    "# input_str = \"This is an incomplete sentence where some words are missing.\"\n",
    "# input_tokens = tokenizer.to_int(input_str)\n",
    "\n",
    "# # Mask \" missing.\". Note that the model performs much better if the masked chunk\n",
    "# # starts with a space.\n",
    "# input_tokens[51:60] = tokenizer.mask_token\n",
    "# print(\"Tokenized string without masked bytes:\")\n",
    "# print(tokenizer.to_string(input_tokens))\n",
    "\n",
    "# #@title Pad and reshape inputs\n",
    "# inputs = input_tokens[None]\n",
    "# input_mask = np.ones_like(inputs)\n",
    "\n",
    "# def pad(max_sequence_length: int, inputs, input_mask):\n",
    "#     input_len = inputs.shape[1]\n",
    "#     assert input_len <= max_sequence_length\n",
    "#     pad_len = max_sequence_length - input_len\n",
    "#     padded_inputs = np.pad(\n",
    "#       inputs,\n",
    "#       pad_width=((0, 0), (0, pad_len)),\n",
    "#       constant_values=tokenizer.pad_token)\n",
    "#     padded_mask = np.pad(\n",
    "#       input_mask,\n",
    "#       pad_width=((0, 0), (0, pad_len)),\n",
    "#       constant_values=0)\n",
    "#     return padded_inputs, padded_mask\n",
    "\n",
    "# inputs, input_mask = pad(args.maxlen, inputs, input_mask)\n",
    "\n",
    "# encoder.eval()\n",
    "# mask = torch.tensor(input_mask)\n",
    "# input_ids = torch.tensor(inputs)\n",
    "# out = encoder.forward(input_ids, mask)\n",
    "\n",
    "# embs = out * mask.unsqueeze(-1) / mask.sum(dim=-1)\n",
    "\n",
    "# logits = torch.matmul(out, encoder.token_embedding.weight.T) + encoder.decoder_token_bias\n",
    "# masked_tokens_predictions = logits[0, 51:60].argmax(dim=-1)\n",
    "# print(\"Greedy predictions:\")\n",
    "# print(masked_tokens_predictions)\n",
    "# print()\n",
    "# print(\"Predicted string:\")\n",
    "# print(tokenizer.to_string(masked_tokens_predictions.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f46123c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:36.176163Z",
     "start_time": "2021-11-30T01:00:36.172891Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.bsz, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=args.bsz, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aefffc22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:36.742398Z",
     "start_time": "2021-11-30T01:00:36.733149Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, args):\n",
    "        super().__init__()\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = args.patch_height, p2 = args.patch_width),\n",
    "            nn.Linear(args.patch_dim, args.embed_dim),\n",
    "        )\n",
    "        self.encoder = encoder\n",
    "        self.numy = args.numy\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        if args.per_label_task:\n",
    "            self.w = nn.Sequential(nn.Linear(args.embed_dim, 2*args.embed_dim), \n",
    "                                   nn.ReLU(), \n",
    "                                   nn.Linear(2*args.embed_dim, 1))\n",
    "        else:\n",
    "            self.w = nn.Linear(args.embed_dim, args.numy)\n",
    "        \n",
    "    def get_device(self):\n",
    "        return list(self.parameters())[0].device\n",
    "    \n",
    "    def forward(self, b):\n",
    "        patch_embs = self.to_patch_embedding(b)\n",
    "        seq_len = patch_embs.size(1)\n",
    "        batch_size = patch_embs.size(0)\n",
    "        \n",
    "        pos_ids = torch.arange(seq_len, device=patch_embs.device).view(1, -1)\n",
    "        pos_embs = self.encoder.position_embedding(pos_ids)\n",
    "        embs = patch_embs + pos_embs\n",
    "        \n",
    "        if args.per_token_decoder:\n",
    "            query_embs = self.encoder.query_position_embedding(pos_ids).repeat(batch_size, 1, 1)\n",
    "            query_mask = None\n",
    "        else:\n",
    "            query_embs = self.encoder.query_task_embedding.weight.repeat(batch_size, 1, 1)\n",
    "            query_mask = None\n",
    "            \n",
    "        embs = self.encoder.perceiver(\n",
    "            inputs=embs,\n",
    "            query=query_embs,\n",
    "            input_mask=None,\n",
    "            query_mask=None\n",
    "        )\n",
    "        \n",
    "        if self.encoder.per_token_decoder:\n",
    "            embs = embs.mean(dim=1)\n",
    "        else:\n",
    "            embs = embs.squeeze()\n",
    "    \n",
    "        out = self.w(self.dropout(embs))\n",
    "        return out.squeeze()\n",
    "    \n",
    "class OvABCELoss(nn.Module):\n",
    "    def __init__(self, args, reduction='mean'):\n",
    "        super(OvABCELoss, self).__init__()\n",
    "        if args.loss_with_logits:\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss(reduction=reduction)\n",
    "        else:\n",
    "            self.criterion = torch.nn.BCELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, model, b):\n",
    "        out = model(b)\n",
    "        targets = torch.zeros((out.shape[0], out.shape[1]+1), device=out.device).scatter_(1, b['y']['inds'], 1)[:, :-1]\n",
    "        loss = self.criterion(out, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b36845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:44.616767Z",
     "start_time": "2021-11-30T01:00:44.613276Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Net(encoder, args)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820f5a54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:45.904894Z",
     "start_time": "2021-11-30T01:00:45.891506Z"
    }
   },
   "outputs": [],
   "source": [
    "optims = [transformers.optimization.AdamW(net.parameters(), **{'lr': args.lr, 'eps': 1e-06, 'weight_decay': 0.01})]\n",
    "total_steps = len(trainloader)*args.n_epochs\n",
    "schedulers = [transformers.get_linear_schedule_with_warmup(optim, num_warmup_steps=int(args.warmup*total_steps), num_training_steps=total_steps) for optim in optims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84958ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:00:50.088881Z",
     "start_time": "2021-11-30T01:00:46.931641Z"
    }
   },
   "outputs": [],
   "source": [
    "net.to(args.device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4512d928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T01:01:09.372081Z",
     "start_time": "2021-11-30T01:01:09.366556Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(net, testloader, epoch=-1):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    t = tqdm(testloader, desc='', leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(t):\n",
    "            inputs, targets = inputs.to(args.device), targets.to(args.device)\n",
    "            with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "                outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            t.set_description(' '.join([str(batch_idx), str(len(testloader)), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total)]))\n",
    "    \n",
    "    acc = 100.*correct/total\n",
    "    loss = test_loss/(batch_idx+1)\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79193c72",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-30T01:01:13.376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72db47bc6454c1d87941687bd8c9ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch: 0, Loss: 0.0:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/08343/nilesh/maverick2/anaconda3/envs/xc/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "best_acc = -100\n",
    "for epoch in range(args.n_epochs):\n",
    "    net.train()\n",
    "    cum_loss = 0; ctr = 0\n",
    "    t = tqdm(trainloader, desc='Epoch: 0, Loss: 0.0', leave=True)\n",
    "          \n",
    "    for b in t:        \n",
    "        for optim in optims: optim.zero_grad()\n",
    "        b = ToD({'input': b[0], 'label': b[1]}, args.device)\n",
    "        with torch.cuda.amp.autocast(enabled=args.amp):\n",
    "            out = net(b['input'])\n",
    "        loss = criterion(out, b['label'])\n",
    "        \n",
    "        if args.amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            for optim in optims: scaler.step(optim)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            for optim in optims: optim.step()\n",
    "                \n",
    "        for sch in schedulers: sch.step()\n",
    "        cum_loss += loss.item()\n",
    "        ctr += 1\n",
    "        t.set_description('Epoch: %d/%d, Loss: %.4E'%(epoch, args.n_epochs, (cum_loss/ctr)), refresh=True)\n",
    "    \n",
    "    print(f'mean loss after epoch {epoch}/{args.n_epochs}: {\"%.4E\"%(cum_loss/ctr)}', flush=True)\n",
    "    if epoch%args.eval_interval == 0 or epoch == (args.n_epochs-1):\n",
    "        test_loss, test_acc = evaluate(net, testloader)\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            print(f'Found new best model with acc: {\"%.2f\"%best_acc}\\n')\n",
    "            with open(f'{OUT_DIR}/log.txt', 'a') as f:\n",
    "                print(f'epoch: {epoch}, test acc: {test_acc}, train loss: {cum_loss/ctr}, test loss: {test_loss}', file=f)\n",
    "            torch.save(net.state_dict(), f'{OUT_DIR}/model.pt')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c53f54a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T00:14:08.332576Z",
     "start_time": "2021-11-30T00:05:14.806256Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a64a5feb854a94a760d709df989326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8536a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T21:38:58.932235Z",
     "start_time": "2021-11-18T21:38:58.913729Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5437/1918826831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnum_params\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for p in model.parameters():\n",
    "    num_params += np.prod(p.shape)\n",
    "\n",
    "num_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vit]",
   "language": "python",
   "name": "conda-env-vit-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
